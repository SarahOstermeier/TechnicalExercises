{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Py9nmahA6_-PdO1RCSoD798ZSic4tz6P",
      "authorship_tag": "ABX9TyModdNMpK2EZHy3//kqw2a+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahOstermeier/TechnicalExercises/blob/main/Arize_Technical_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planning\n",
        "\n",
        "**Objective:**  Build a RAG application\n",
        "\n",
        "## Approach\n",
        "\n",
        "**Format:** Jupyter Notebook (Google Colab)  \n",
        "**Stretch Goal:** Optimize performance (primary), Build UX (secondary)  \n",
        "**Framework:** Langchain or DSPy  \n",
        "**LLM Provider:** Huggingface or Mistral  \n",
        "**Dataset:** [OpenStax](https://openstax.org/subjects)\n",
        "\n",
        "****\n",
        "\n",
        "## Requirements\n",
        "\n",
        "**A working RAG app with some interface for Q&A**  \n",
        "* ~75-80% of the time, 2-3 hours <br>\n",
        "\n",
        "\n",
        "**Thorough documentation**  \n",
        "\n",
        "* Clear setup instructions - make it so anyone can follow in your footsteps\n",
        "* Tell us why you picked your tools\n",
        "* Share what worked, what didn't, and how you dealt with it\n",
        "* What would you do next if you had more time?\n",
        "* ~20-25% of your time, 1 hour\n",
        "\n",
        "****\n",
        "\n",
        "## Tips\n",
        "\n",
        "* Use those quickstart tools - no need to reinvent the wheel\n",
        "* Document as you go - future you will thank you\n",
        "* LLMs are your friend here, don’t be afraid to use them to help, just be sure you take the time to really understand what they tell you.\n",
        "* Hit a wall? Don't spin your wheels - reach out!\n",
        "* Keep it focused - better to nail the basics than half-finish three extra features"
      ],
      "metadata": {
        "id": "rnvVjlbV2yqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run this notebook"
      ],
      "metadata": {
        "id": "i8JPcqF0NNeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My process\n",
        "\n",
        "## Planning\n",
        "\n",
        "### Appraoch and Tools\n",
        "* I decided to work in Google Colab since it is a tool I am familiar with and will allow me to get started quickly without much setup.\n",
        "* As my RAG framework I chose DSPy, as I'm interested interested in trying out DSPy Optimizers and thought this would be a good opportunity to do so.  \n",
        "* Related to the above, my stretch goal is to optimize performance.\n",
        "* I'll be using HuggingFace or Mistral as my LLM provider, as I already have accounts for both and can access easily.\n",
        "\n",
        "### Use Case and Dataset Selection\n",
        "I started a project on Claude and provided the exercise instructions and the Jupyter Notebook I started as project content. I used Claude to brainstorm project ideas and related datasets and eventually decided to build a RAG tool to query textbooks, using documents from [OpenStax](https://openstax.org/subjects) as my dataset.\n",
        "\n",
        "## Implementation\n",
        "*italicized text*"
      ],
      "metadata": {
        "id": "6y57z-iJJnGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment set up"
      ],
      "metadata": {
        "id": "C3Xx0TJmNVba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import relevant libraries\n",
        "\n",
        "[**DSPy:**](https://dspy.ai/) Framework for RAG application. DSPy provides a \"prompts as code\" library, enabling AI developers to standardize, modularize, and optimize their AI applicatins.\n",
        "\n",
        "**PyDF2:** To extract text from PDFs\n",
        "\n"
      ],
      "metadata": {
        "id": "6ctR71kIIQ-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install dspy\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCeCZQaDUF57",
        "outputId": "a591178e-9afc-4872-8a6a-ba3030df6d33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: dspy in /usr/local/lib/python3.11/dist-packages (2.6.10)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.4.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from dspy) (1.61.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from dspy) (2024.11.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from dspy) (5.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dspy) (4.67.1)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.14.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dspy) (2.32.3)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from dspy) (4.2.1)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.5)\n",
            "Requirement already satisfied: magicattr~=0.1.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.1.6)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.59.8 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.62.1)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from dspy) (5.6.3)\n",
            "Requirement already satisfied: json-repair in /usr/local/lib/python3.11/dist-packages (from dspy) (0.39.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (9.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from dspy) (3.7.1)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.0.8)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from dspy) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.14.6->dspy) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (8.1.8)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (8.6.1)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (4.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (1.0.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->dspy) (3.0.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (2025.1.31)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (2.0.38)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2025.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->dspy) (1.3.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.18.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.59.8->dspy) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm<2.0.0,>=1.59.8->dspy) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.59.8->dspy) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dspy) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->dspy) (3.1.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import langchain\n",
        "import dspy"
      ],
      "metadata": {
        "id": "lTJAiGhMJlu_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model set up\n",
        "In this tutorial I'll be accessing models thorugh [Mistral](https://mistral.ai/) and through Huggingface's ([ Serverless inference API](https://huggingface.co/docs/api-inference/index), both of which can be used for free, with some limitiations. The API calls will be made through **DSPy**, which [integrates with a wide range of model providers](https://dspy.ai/)"
      ],
      "metadata": {
        "id": "oKbOyMZv6xRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Model provider API keys as environment variables."
      ],
      "metadata": {
        "id": "u7DV1JoRyFOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out if API keys are not saved in your google colab userdata\n",
        "from google.colab import userdata\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "# os.environ[\"HUGGINGFACE_API_KEY\"] = userdata.get('HUGGINGFACE_API_KEY')\n",
        "\n",
        "## Uncomment and add API keys here if they are not saved in your google colab userdata\n",
        "# os.environ[\"MISTRAL_API_KEY\"] = 'YOUR_MISTRAL_API_KEY'\n",
        "# os.environ[\"HUGGINGFACE_API_KEY\"] = 'YOUR_HUGGINGFACE_API_KEY"
      ],
      "metadata": {
        "id": "4kcTkbYOyCR9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the LLM endpoint with with DSPy."
      ],
      "metadata": {
        "id": "Vx78m6CH5W1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = dspy.LM('mistral/mistral-small-latest')\n",
        "dspy.configure(lm=lm)"
      ],
      "metadata": {
        "id": "s4zfgXMJ29W7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the endpoint.\n"
      ],
      "metadata": {
        "id": "BNvZE-3J3dvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test!\"}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiyaMttr3iqZ",
        "outputId": "60b28a62-e1ba-4970-e938-b2efaf0017e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"This is a test! How can I assist you further? Let's test something if you'd like. How about I say something and you respond with the first word that comes to your mind? I'll start:\\n\\nCat\\n\\n(What word does that make you think of?)\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "oFWlcKaFOCkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection and Processing\n",
        "\n",
        "Started by downloading a couple of textbook and access them directly from google drive. Will add web scraping later if there's time\n"
      ],
      "metadata": {
        "id": "k-NRMdl7OH_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the PDFs\n",
        "\n",
        "**Approach:** I used Claude to generate the initial data processing functions. I wanted to save metadata such as page number and source title for each text chunk to allow for citations in the LLM responses.\n",
        "I used used LangChain's RecursiveCharacterTextSplitter function for text chunking and started with the following parameters:\n",
        "\n",
        "  chunk_size: 1000  \n",
        "  chunk_overlap: 200\n",
        "\n",
        "\n",
        "**Problems:**  \n",
        "Initially I did this by first split the documents by page and then chunked the text within each page. However, I ran into some compatibility issues with the data structures the Claude-generated functions produced (Claude does not have access to recent libarary updates). After reviewing more recent LangChain and DSPy documentation, I updated my approach to take advantage of some simplified new functions and modified my data processing functions to do page splitting and chunking in one step to improve efficiency.\n",
        "\n",
        "\n",
        "**Possible future improvement:** The current approach does not preserve chapter/section structure in the textbook or content such as images and structured tables. In a more sophisticated implemenation it might be worth doing some more structured data splitting and including more details such as chapter and section in the metadata."
      ],
      "metadata": {
        "id": "y5iMLQMebYyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_drive_dir = \"/content/drive/MyDrive/Colab Notebooks/Arize RAG Exercise\"\n",
        "project_data_folder = \"Data\""
      ],
      "metadata": {
        "id": "izgiUAooS3BI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Old Approach"
      ],
      "metadata": {
        "id": "SM1lEMe6yQbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file, along with page numbers.\"\"\"\n",
        "    documents = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Get basic information\n",
        "        title = os.path.basename(pdf_path).replace('.pdf', '')\n",
        "        total_pages = len(pdf_reader.pages)\n",
        "\n",
        "        # Process each page\n",
        "        for page_num in range(total_pages):\n",
        "            # Extract text from the page\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Skip empty pages\n",
        "            if not text or len(text.strip()) < 50:  # Skip pages with litter or no text\n",
        "                continue\n",
        "\n",
        "            # Create a document with metadata for citation\n",
        "            documents.append({\n",
        "                'content': text,\n",
        "                'metadata': {\n",
        "                    'source': title,\n",
        "                    'page': page_num + 1,\n",
        "                    'total_pages': total_pages\n",
        "                }\n",
        "            })\n",
        "\n",
        "    return documents\n",
        "\n",
        "def process_pdfs_in_directory(directory):\n",
        "    \"\"\"Process all PDFs in a directory.\"\"\"\n",
        "    all_documents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                documents = extract_text_from_pdf(file_path)\n",
        "                all_documents.extend(documents)\n",
        "                print(f\"Processed {filename}: {len(documents)} pages extracted\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    return all_documents"
      ],
      "metadata": {
        "id": "MC22tPkdT-bC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load my documents\n",
        "pdf_directory = os.path.join(project_drive_dir, project_data_folder)\n",
        "documents = process_pdfs_in_directory(pdf_directory)\n",
        "\n",
        "# Save to CSV for later use\n",
        "df = pd.DataFrame(documents)\n",
        "textbook_content_path = os.path.join(project_drive_dir, 'textbook_content.csv')\n",
        "df.to_csv(textbook_content_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lar9TxmVltO",
        "outputId": "3dc8b51f-f33e-4ab1-f31a-b069acc24176"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed ConceptsofBiology-WEB.pdf: 612 pages extracted\n",
            "Processed Introduction_to_Behavioral_Neuroscience-WEB.pdf: 912 pages extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### New Approach"
      ],
      "metadata": {
        "id": "DtpMtgjqyZBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_chunk_pdf(pdf_path, text_splitter):\n",
        "    \"\"\"Extract text from a PDF file, split into chunks, and maintain metadata.\"\"\"\n",
        "    chunked_documents = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Get basic information\n",
        "        title = os.path.basename(pdf_path).replace('.pdf', '')\n",
        "        total_pages = len(pdf_reader.pages)\n",
        "\n",
        "        # Process each page\n",
        "        for page_num in range(total_pages):\n",
        "            # Extract text from the page\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Skip empty pages\n",
        "            if not text or len(text.strip()) < 50:  # Skip pages with little or no text\n",
        "                continue\n",
        "\n",
        "            # Create metadata for this page\n",
        "            metadata = {\n",
        "                'source': title,\n",
        "                'page': page_num + 1,\n",
        "                'total_pages': total_pages\n",
        "            }\n",
        "\n",
        "            # Split this page's text into chunks\n",
        "            page_chunks = text_splitter.split_text(text)\n",
        "\n",
        "            # Create a document for each chunk with proper metadata\n",
        "            for chunk_idx, chunk in enumerate(page_chunks):\n",
        "                chunked_documents.append({\n",
        "                    'content': chunk,\n",
        "                    'metadata': {\n",
        "                        **metadata,\n",
        "                        'chunk_id': f\"{page_num}-{chunk_idx}\"\n",
        "                    }\n",
        "                })\n",
        "\n",
        "    return chunked_documents\n",
        "\n",
        "def process_pdfs_in_directory(directory, text_splitter):\n",
        "    \"\"\"Process all PDFs in a directory, splitting into chunks with metadata.\"\"\"\n",
        "    all_chunked_documents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                chunked_docs = extract_and_chunk_pdf(file_path, text_splitter)\n",
        "                all_chunked_documents.extend(chunked_docs)\n",
        "                print(f\"Processed {filename}: {len(chunked_docs)} chunks extracted\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    return all_chunked_documents"
      ],
      "metadata": {
        "id": "opeFTqWOycQw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Set up the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False\n",
        ")\n",
        "\n",
        "# Process all PDFs and get chunks with metadata\n",
        "pdf_directory = os.path.join(project_drive_dir, project_data_folder)\n",
        "chunked_documents = process_pdfs_in_directory(pdf_directory, text_splitter)\n",
        "\n",
        "# Save to CSV for later use if needed\n",
        "chunked_df = pd.DataFrame(chunked_documents)\n",
        "textbook_chunks_path = os.path.join(project_drive_dir, 'textbook_chunks.csv')\n",
        "chunked_df.to_csv(textbook_chunks_path, index=False)\n",
        "\n",
        "# Extract just the content for the DSPy retriever\n",
        "chunk_texts = [doc['content'] for doc in chunked_documents]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaBnIlhHzJsI",
        "outputId": "267bdf59-224a-4346-ac63-b6ed99344ae2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed ConceptsofBiology-WEB.pdf: 2319 chunks extracted\n",
            "Processed Introduction_to_Behavioral_Neuroscience-WEB.pdf: 3495 chunks extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSPy Setup for RAG\n",
        "\n",
        "**Approach:** I configured the embedder and retriever in DSPY, using the mistral-embed model, retrieving 5 documents per query.\n",
        "\n",
        "**Problems:**\n",
        "* Claude is pretty out of date with DSPy's current capabilities, so I was not able to rely heavily on generated code for this part.\n",
        "* I had to set a pretty small batch size for the Embedder to accomodate Mistral's token limit, since I didn't want to reduce the chunk size quite yet.\n",
        "\n",
        "**Possible future improvement:** Experiment with different embedding models to optimize for performance, optimize chunk size"
      ],
      "metadata": {
        "id": "dzJGctgKOlqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U faiss-cpu  # or faiss-gpu if you have a GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaghPIZS5y5T",
        "outputId": "ba82ef8b-10ce-41a5-cb45-e10b6c616dc2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the embedder and retriever\n",
        "embedder = dspy.Embedder(model='mistral/mistral-embed', batch_size=25)\n",
        "retriever = dspy.retrievers.Embeddings(\n",
        "    corpus=chunk_texts,\n",
        "    embedder=embedder\n",
        "    )\n",
        "\n",
        "# Save embeddings to avoid the need to generate them again\n",
        "embeddings_path = os.path.join(project_drive_dir, 'corpus_embeddings.npy')\n",
        "np.save(embeddings_path, retriever.corpus_embeddings)\n",
        "\n",
        "# # To re-load embeddings\n",
        "# loaded_embeddings = np.load(embeddings_path)\n"
      ],
      "metadata": {
        "id": "uEYorw8S4ZTG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure DSPy\n",
        "dspy.settings.configure(lm=lm, rm=retriever)"
      ],
      "metadata": {
        "id": "d3_ZyCD1AW1K"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define signatures for your RAG modules\n",
        "class EducationalQuery(dspy.Signature):\n",
        "    \"\"\"Query an educational assistant about textbook content.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    context = dspy.InputField(desc=\"Retrieved passages from textbooks\")\n",
        "    answer = dspy.OutputField(desc=\"Comprehensive answer based on the retrieved information\")\n",
        "    sources = dspy.OutputField(desc=\"The sources used to answer the question\")\n",
        "\n",
        "# Create educational assistant module\n",
        "class EducationalAssistant(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.retrieve = dspy.Retrieve(k=5)  # Retrieve 5 most relevant passages\n",
        "        self.generate = dspy.ChainOfThought(EducationalQuery)\n",
        "\n",
        "    def forward(self, question):\n",
        "        retrieved = self.retrieve(question)\n",
        "        answer = self.generate(\n",
        "            question=question,\n",
        "            context=\"\\n\\n\".join(retrieved.passages)\n",
        "        )\n",
        "        return {\n",
        "            \"answer\": answer.answer,\n",
        "            \"sources\": answer.sources\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Ys2TbYerRoDI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your assistant\n",
        "assistant = EducationalAssistant()\n",
        "\n",
        "# Example usage\n",
        "response = assistant(\"What is cellular respiration?\")\n",
        "print(response[\"answer\"])\n",
        "print(\"\\nSources:\")\n",
        "print(response[\"sources\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "16rypvX-Rtml",
        "outputId": "f8025531-7657-4310-d047-34a55d7a5e35"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Embeddings.__call__() got an unexpected keyword argument 'k'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2e91b404cde4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is cellular respiration?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSources:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/utils/callback.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# If no callbacks are provided, just call the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/primitives/program.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnamed_predictors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-584d1130ddfe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mretrieved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         answer = self.generate(\n\u001b[1;32m     19\u001b[0m             \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/utils/callback.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# If no callbacks are provided, just call the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/retrieve/retrieve.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwith_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     def forward(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dspy/retrieve/retrieve.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No RM is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mpassages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Embeddings.__call__() got an unexpected keyword argument 'k'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define signatures for your RAG modules\n",
        "class EducationalQuery(dspy.Signature):\n",
        "    \"\"\"Query an educational assistant about textbook content.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    context = dspy.InputField(desc=\"Retrieved passages from textbooks\")\n",
        "    answer = dspy.OutputField(desc=\"Comprehensive answer based on the retrieved information\")\n",
        "    sources = dspy.OutputField(desc=\"The sources used to answer the question\")\n",
        "\n",
        "# Create educational assistant module\n",
        "class EducationalAssistant(dspy.Module):\n",
        "    def __init__(self):\n",
        "        self.retrieve = dspy.retrieve()  # Retrieve 5 most relevant passages\n",
        "        self.generate = dspy.ChainOfThought(EducationalQuery)\n",
        "\n",
        "    def forward(self, question):\n",
        "        # Retrieve relevant passages\n",
        "        retrieved = self.retrieve(question)\n",
        "        passages = retrieved.passages\n",
        "\n",
        "        # Include metadata in prompts\n",
        "        context_with_citations = []\n",
        "        for i, passage in enumerate(passages):\n",
        "            # Find the corresponding metadata for this passage\n",
        "            metadata = chunked_documents[retrieved.indices[i]]['metadata']\n",
        "            citation = f\"[{metadata['source']}, Page {metadata['page']}]\"\n",
        "            context_with_citations.append(f\"{passage} {citation}\")\n",
        "\n",
        "        # Format the context from retrieved passages\n",
        "        context = \"\\n\\n\".join(context_with_citations)\n",
        "\n",
        "        answer = self.generate(\n",
        "            question=question,\n",
        "            context=context)\n",
        "\n",
        "        return {\n",
        "            \"answer\": answer.answer,\n",
        "            \"sources\": answer.sources\n",
        "        }\n",
        "\n",
        "# Initialize your assistant\n",
        "assistant = EducationalAssistant()\n",
        "\n",
        "# Example usage\n",
        "response = assistant(\"What is cellular respiration?\")\n",
        "print(response[\"answer\"])\n",
        "print(\"\\nSources:\")\n",
        "print(response[\"sources\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "_n7TSUkgISvA",
        "outputId": "fc1f59dd-9b72-452f-f392-b93f61a17e39"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-fa7892b871f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Initialize your assistant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0massistant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEducationalAssistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-fa7892b871f1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEducationalAssistant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Retrieve 5 most relevant passages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdspy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChainOfThought\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEducationalQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Educational Domain Knowledge"
      ],
      "metadata": {
        "id": "xRM11j4wOwwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "URVVfXD2O7EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Usage Disclosure\n",
        "\n",
        "The following LLM-based assistants were used in the development of this notebook:\n",
        "\n",
        "Claude 3.7 Sonnet for:\n",
        "* Use case brainstorming and dataset selection\n",
        "\n",
        "\n",
        "\n",
        "## Authorship\n",
        "All core components, concepts, and technical implementation of this notebook were authored by Sarah Ostermeier. LLM assistance was limited to the specific tasks listed above.\n"
      ],
      "metadata": {
        "id": "KdZzVVlDL0m4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFKdBWE62KB7"
      },
      "outputs": [],
      "source": []
    }
  ]
}