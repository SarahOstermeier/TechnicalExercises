{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MpN1JW0Z43T02M-qgZNGAZG8CPrqMhNa",
      "authorship_tag": "ABX9TyOCkJAMSjQ4/xhfOhWPQbJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahOstermeier/TechnicalExercises/blob/main/Arize_Technical_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planning\n",
        "\n",
        "**Objective:**  Build a RAG application\n",
        "\n",
        "## Approach\n",
        "\n",
        "**Format:** Jupyter Notebook (Google Colab)  \n",
        "**Stretch Goal:** Optimize performance (primary), Build UX (secondary)  \n",
        "**Framework:** Langchain or DSPy  \n",
        "**LLM Provider:** Huggingface or Mistral  \n",
        "**Dataset:** [OpenStax](https://openstax.org/subjects)\n",
        "\n",
        "****\n",
        "\n",
        "## Requirements\n",
        "\n",
        "**A working RAG app with some interface for Q&A**  \n",
        "* ~75-80% of the time, 2-3 hours <br>\n",
        "\n",
        "\n",
        "**Thorough documentation**  \n",
        "\n",
        "* Clear setup instructions - make it so anyone can follow in your footsteps\n",
        "* Tell us why you picked your tools\n",
        "* Share what worked, what didn't, and how you dealt with it\n",
        "* What would you do next if you had more time?\n",
        "* ~20-25% of your time, 1 hour\n",
        "\n",
        "****\n",
        "\n",
        "## Tips\n",
        "\n",
        "* Use those quickstart tools - no need to reinvent the wheel\n",
        "* Document as you go - future you will thank you\n",
        "* LLMs are your friend here, don’t be afraid to use them to help, just be sure you take the time to really understand what they tell you.\n",
        "* Hit a wall? Don't spin your wheels - reach out!\n",
        "* Keep it focused - better to nail the basics than half-finish three extra features"
      ],
      "metadata": {
        "id": "rnvVjlbV2yqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run this notebook"
      ],
      "metadata": {
        "id": "i8JPcqF0NNeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My process\n",
        "\n",
        "## Planning\n",
        "\n",
        "### Appraoch and Tools\n",
        "* I decided to work in Google Colab since it is a tool I am familiar with and will allow me to get started quickly without much setup.\n",
        "* As my RAG framework I chose DSPy, as I'm interested interested in trying out DSPy Optimizers and thought this would be a good opportunity to do so.  \n",
        "* Related to the above, my stretch goal is to optimize performance.\n",
        "* I'll be using HuggingFace or Mistral as my LLM provider, as I already have accounts for both and can access easily.\n",
        "\n",
        "### Use Case and Dataset Selection\n",
        "I started a project on Claude and provided the exercise instructions and the Jupyter Notebook I started as project content. I used Claude to brainstorm project ideas and related datasets and eventually decided to build a RAG tool to query textbooks, using documents from [OpenStax](https://openstax.org/subjects) as my dataset.\n",
        "\n",
        "## Implementation\n",
        "*italicized text*"
      ],
      "metadata": {
        "id": "6y57z-iJJnGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment set up"
      ],
      "metadata": {
        "id": "C3Xx0TJmNVba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import relevant libraries\n",
        "\n",
        "[**DSPy:**](https://dspy.ai/) Framework for RAG application. DSPy provides a \"prompts as code\" library, enabling AI developers to standardize, modularize, and optimize their AI applicatins.\n",
        "\n",
        "**PyDF2:** To extract text from PDFs\n",
        "\n"
      ],
      "metadata": {
        "id": "6ctR71kIIQ-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install dspy\n",
        "!pip install langchain\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCeCZQaDUF57",
        "outputId": "7c81863a-5cc4-4640-ae46-14b335e40010"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: dspy in /usr/local/lib/python3.11/dist-packages (2.6.10)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.4.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from dspy) (1.61.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from dspy) (2024.11.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from dspy) (5.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dspy) (4.67.1)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.14.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dspy) (2.32.3)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from dspy) (4.2.1)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.5)\n",
            "Requirement already satisfied: magicattr~=0.1.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.1.6)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.59.8 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.62.1)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from dspy) (5.6.3)\n",
            "Requirement already satisfied: json-repair in /usr/local/lib/python3.11/dist-packages (from dspy) (0.39.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (9.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from dspy) (3.7.1)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.0.8)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from dspy) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.14.6->dspy) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (8.1.8)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (8.5.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (4.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (1.0.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->dspy) (3.0.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (2025.1.31)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (2.0.38)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2025.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->dspy) (1.3.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.18.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.59.8->dspy) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm<2.0.0,>=1.59.8->dspy) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.59.8->dspy) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dspy) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->dspy) (3.1.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (0.6.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.11)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.18.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb) (0.46.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.68.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import langchain\n",
        "import dspy"
      ],
      "metadata": {
        "id": "lTJAiGhMJlu_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model set up\n",
        "In this tutorial I'll be accessing models thorugh [Mistral](https://mistral.ai/) and through Huggingface's ([ Serverless inference API](https://huggingface.co/docs/api-inference/index), both of which can be used for free, with some limitiations. The API calls will be made through **DSPy**, which [integrates with a wide range of model providers](https://dspy.ai/)"
      ],
      "metadata": {
        "id": "oKbOyMZv6xRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Model provider API keys as environment variables."
      ],
      "metadata": {
        "id": "u7DV1JoRyFOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out if API keys are not saved in your google colab userdata\n",
        "from google.colab import userdata\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = userdata.get('HUGGINGFACE_API_KEY')\n",
        "\n",
        "## Uncomment and add API keys here if they are not saved in your google colab userdata\n",
        "# os.environ[\"MISTRAL_API_KEY\"] = 'YOUR_MISTRAL_API_KEY'\n",
        "# os.environ[\"HUGGINGFACE_API_KEY\"] = 'YOUR_HUGGINGFACE_API_KEY"
      ],
      "metadata": {
        "id": "4kcTkbYOyCR9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the LLM endpoint with with DSPy."
      ],
      "metadata": {
        "id": "Vx78m6CH5W1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = dspy.LM('mistral/mistral-small-latest')\n",
        "dspy.configure(lm=lm)"
      ],
      "metadata": {
        "id": "s4zfgXMJ29W7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the endpoint.\n"
      ],
      "metadata": {
        "id": "BNvZE-3J3dvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test!\"}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiyaMttr3iqZ",
        "outputId": "5d4c2b3d-0902-48dd-87e6-de6567e536b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"This is a test! How can I assist you further? Let's test something if you'd like. How about I say something and you respond with the first word that comes to your mind? I'll start:\\n\\nCat\\n\\n(What word does that make you think of?)\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "oFWlcKaFOCkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection and Processing\n",
        "\n",
        "Started by downloading a couple of textbook and access them directly from google drive. Will add web scraping later if there's time\n"
      ],
      "metadata": {
        "id": "k-NRMdl7OH_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the PDFs\n",
        "\n",
        "**Approach:** I used Claude to generate the initial data processing functions. I wanted to save metadata such as page number and source title for each text chunk to allow for citations in the LLM responses.\n",
        "I used used LangChain's RecursiveCharacterTextSplitter function for text chunking and started with the following parameters:\n",
        "\n",
        "  chunk_size: 1000  \n",
        "  chunk_overlap: 200\n",
        "\n",
        "\n",
        "**Problems:**  \n",
        "Initially I did this by first split the documents by page and then chunked the text within each page. However, I ran into some compatibility issues with the data structures the Claude-generated functions produced (Claude does not have access to recent libarary updates). After reviewing more recent LangChain and DSPy documentation, I updated my approach to take advantage of some simplified new functions and modified my data processing functions to do page splitting and chunking in one step to improve efficiency.\n",
        "\n",
        "\n",
        "**Possible future improvement:** The current approach does not preserve chapter/section structure in the textbook or content such as images and structured tables. In a more sophisticated implemenation it might be worth doing some more structured data splitting and including more details such as chapter and section in the metadata."
      ],
      "metadata": {
        "id": "y5iMLQMebYyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import os\n",
        "import uuid\n",
        "\n",
        "def process_documents_for_chroma(pdf_directory):\n",
        "    \"\"\"\n",
        "    Process PDF documents and prepare them for ChromaDB ingestion\n",
        "    \"\"\"\n",
        "    documents = []\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "\n",
        "    for filename in os.listdir(pdf_directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(pdf_directory, filename)\n",
        "            try:\n",
        "                # Extract text with metadata\n",
        "                doc_chunks = extract_and_chunk_pdf(file_path)\n",
        "\n",
        "                # Process each chunk\n",
        "                for chunk in doc_chunks:\n",
        "                    documents.append(chunk['content'])\n",
        "                    metadatas.append(chunk['metadata'])\n",
        "                    ids.append(str(uuid.uuid4()))\n",
        "\n",
        "                print(f\"Processed {filename}: {len(doc_chunks)} chunks extracted\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    return documents, metadatas, ids\n",
        "\n",
        "def extract_and_chunk_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file, split into chunks, and maintain metadata.\"\"\"\n",
        "    chunked_documents = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Get basic information\n",
        "        title = os.path.basename(pdf_path).replace('.pdf', '')\n",
        "        total_pages = len(pdf_reader.pages)\n",
        "\n",
        "        # Process each page\n",
        "        for page_num in range(total_pages):\n",
        "            # Extract text from the page\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Skip empty pages\n",
        "            if not text or len(text.strip()) < 50:  # Skip pages with little or no text\n",
        "                continue\n",
        "\n",
        "            # Create metadata for this page\n",
        "            metadata = {\n",
        "                'source': title,\n",
        "                'page': page_num + 1,\n",
        "                'total_pages': total_pages\n",
        "            }\n",
        "\n",
        "            # Split this page's text into chunks (simple character-based chunking)\n",
        "            chunk_size = 1000\n",
        "            overlap = 200\n",
        "            text_length = len(text)\n",
        "\n",
        "            start = 0\n",
        "            chunk_idx = 0\n",
        "\n",
        "            while start < text_length:\n",
        "                end = min(start + chunk_size, text_length)\n",
        "\n",
        "                # If this is not the first chunk, include some overlap\n",
        "                if start > 0:\n",
        "                    start = max(0, start - overlap)\n",
        "\n",
        "                chunk_text = text[start:end]\n",
        "\n",
        "                # Add the chunk with metadata\n",
        "                chunked_documents.append({\n",
        "                    'content': chunk_text,\n",
        "                    'metadata': {\n",
        "                        **metadata,\n",
        "                        'chunk_id': f\"{page_num}-{chunk_idx}\"\n",
        "                    }\n",
        "                })\n",
        "\n",
        "                # Move to next chunk\n",
        "                start = end\n",
        "                chunk_idx += 1\n",
        "\n",
        "    return chunked_documents\n"
      ],
      "metadata": {
        "id": "0PnzNXETpyfT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_drive_dir = \"/content/drive/MyDrive/Colab Notebooks/Arize RAG Exercise\"\n",
        "project_data_folder = \"Data\""
      ],
      "metadata": {
        "id": "izgiUAooS3BI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents, metadatas, ids = process_documents_for_chroma(os.path.join(project_drive_dir, project_data_folder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGEktwoHp9wy",
        "outputId": "9b32e038-38ca-41fc-9dcf-a19436e09834"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Introduction_to_Behavioral_Neuroscience-WEB.pdf: 3091 chunks extracted\n",
            "Processed ConceptsofBiology-WEB.pdf: 2057 chunks extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ChromaDB\n",
        "persist_dir = \"/content/drive/MyDrive/Colab Notebooks/Arize RAG Exercise/Vector_Data\"\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=persist_dir)"
      ],
      "metadata": {
        "id": "v2gsAyIzGTU5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new collection (or get existing one)\n",
        "# You can choose the embedding function based on your needs\n",
        "embedding_function = embedding_functions.DefaultEmbeddingFunction()\n",
        "collection_name = \"textbooks\"\n",
        "collection = chroma_client.create_collection(\n",
        "    name=collection_name,\n",
        "    embedding_function=embedding_function\n",
        ")"
      ],
      "metadata": {
        "id": "0rDcuaxsGh0w"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add documents to the collection\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")\n"
      ],
      "metadata": {
        "id": "jGND6updrjzF"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection.peek()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOAiQS--wyH7",
        "outputId": "a375cb4d-76df-4076-d54a-be6851b021ad"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['23b101d2-4a10-4854-9f57-78da025c62b9',\n",
              "  '84a6e605-2e92-4a54-8b4b-51dc57fd7d4e',\n",
              "  'ba804869-baa8-4b81-a053-647ba79d2eb2',\n",
              "  'd063c29c-bc74-4382-9cb7-7a5be9a17404',\n",
              "  'c500af61-7fd1-4815-afb0-e2773a9bf91d',\n",
              "  '83e79544-a83e-4242-b315-5a4a8d29abc0',\n",
              "  '43f1517a-a48d-488b-9f71-575bfeab3e6b',\n",
              "  '98a2d589-3660-4dc6-a543-1e9c0333052d',\n",
              "  '1d7b9d9f-f64f-44de-aa1c-fbe5c22634b6',\n",
              "  'bc606aec-c280-431d-b86c-199e8af3922f'],\n",
              " 'embeddings': array([[ 0.03137285, -0.04941513,  0.06488694, ...,  0.11583801,\n",
              "         -0.06308822, -0.0459537 ],\n",
              "        [-0.02444978, -0.03617395, -0.04078932, ..., -0.0215064 ,\n",
              "          0.01420389,  0.01402987],\n",
              "        [-0.02652909, -0.01673495, -0.06976717, ..., -0.00866218,\n",
              "          0.02032539,  0.00517704],\n",
              "        ...,\n",
              "        [ 0.01859296, -0.05630921, -0.02378133, ...,  0.07191464,\n",
              "         -0.0512832 ,  0.0735969 ],\n",
              "        [-0.11047661, -0.0109393 ,  0.00610211, ..., -0.01831879,\n",
              "          0.08802343, -0.03965753],\n",
              "        [-0.04421325, -0.00484203, -0.04020387, ...,  0.01531688,\n",
              "          0.12005425,  0.05908196]]),\n",
              " 'documents': ['\\t\\n\\t      Introduction to Behavioral Neuroscience          SENIOR CONTRIBUTING AUTHORS ELIZABETH D. KIRBY, THE OHIO STATE UNIVERSITY MELISSA J. GLENN, COLBY COLLEGE NOAH J. SANDSTROM, WILLIAMS COLLEGE CHRISTINA L. WILLIAMS, DUKE UNIVERSITY         \\n',\n",
              "  '\\t\\n\\tOpenStax Rice University 6100 Main Street MS-375 Houston, Texas 77005  To learn more about OpenStax, visit https://openstax.org. Individual print copies and bulk orders can be purchased through our website.  ©2024 Rice University. Textbook content produced by OpenStax is licensed under a Creative Commons Attribution Non-Commercial ShareAlike 4.0 International License (CC BY-NC-SA 4.0). Under this license, any user of this textbook or the textbook contents herein can share, remix, and build upon the content for noncommercial purposes only. Any adaptations must be shared under the same type of license. In any case of sharing the original or adapted material, whether in whole or in part, the user must provide proper attribution as follows:  - If you noncommercially redistribute this textbook in a digital format (including but not limited to PDF and HTML), then you must retain on every page the following attribution:  “Access for free at openstax.org.” - If you noncommercially redistrib',\n",
              "  'ok in a digital format (including but not limited to PDF and HTML), then you must retain on every page the following attribution:  “Access for free at openstax.org.” - If you noncommercially redistribute this textbook in a print format, then you must include on every physical page the following attribution: “Access for free at openstax.org.” - If you noncommercially redistribute part of this textbook, then you must retain in every digital format page view (including but not limited to PDF and HTML) and on every physical printed page the following attribution:  “Access for free at openstax.org.” - If you use this textbook as a bibliographic reference, please include https://openstax.org/details/books/introduction-behavioral-neuroscience in your citation.  For questions regarding this licensing, please contact support@openstax.org.  Trademarks The OpenStax name, OpenStax logo, OpenStax book covers, OpenStax CNX name, OpenStax CNX logo, OpenStax Tutor name, Openstax Tutor logo, Connexions name, Connexions logo, Rice University name, and Rice University logo are not subject to the license and may not be reproduced without the prior and express written consent of Rice University.  Kenda',\n",
              "  ' name, Connexions logo, Rice University name, and Rice University logo are not subject to the license and may not be reproduced without the prior and express written consent of Rice University.  Kendall Hunt and the Kendall Hunt Logo are trademarks of Kendall Hunt. The Kendall Hunt mark is registered in the United States, Canada, and the European Union. These trademarks may not be used without the prior and express written consent of Kendall Hunt.   COLOR PAPERBACK BOOK ISBN-13 979-8-3851-6183-6 B&W PAPERBACK BOOK ISBN-13 979-8-3851-6184-3 DIGITAL VERSION ISBN-13 978-1-961584-57-0 ORIGINAL PUBLICATION YEAR 2024 1 2 3 4 5 6 7 8 9 10 RS 24    ',\n",
              "  '\\t\\n\\tOPENSTAX  OpenStax provides free, peer-reviewed, openly licensed textbooks for introductory college and Advanced Placement® courses and low-cost, personalized courseware that helps students learn. A nonprofit ed tech initiative based at Rice University, we’re committed to helping students access the tools they need to complete their courses and meet their educational goals.  RICE UNIVERSITY  OpenStax is an initiative of Rice University. As a leading research university with a distinctive commitment to undergraduate education, Rice University aspires to path-breaking research, unsurpassed teaching, and contributions to the betterment of our world. It seeks to fulfill this mission by cultivating a diverse community of learning and discovery that produces leaders across the spectrum of human endeavor.    PHILANTHROPIC SUPPORT  OpenStax is grateful for the generous philanthropic partners who advance our mission to improve educational access and learning for everyone. To see the impact o',\n",
              "  'an endeavor.    PHILANTHROPIC SUPPORT  OpenStax is grateful for the generous philanthropic partners who advance our mission to improve educational access and learning for everyone. To see the impact of our supporter community and our most updated list of partners, please visit openstax.org/foundation.  Arnold Ventures Chan Zuckerberg Initiative Chegg, Inc. Arthur and Carlyse Ciocca Charitable Foundation Digital Promise Ann and John Doerr Bill & Melinda Gates Foundation Girard Foundation Google Inc. The William and Flora Hewlett Foundation The Hewlett-Packard Company Intel Inc. Rusty and John Jaggers The Calvin K. Kazanjian Economics Foundation Charles Koch Foundation Leon Lowenstein Foundation, Inc. The Maxfield Foundation Burt and Deedee McMurtry Michelson 20MM Foundation National Science Foundation The Open Society Foundations Jumee Yhu and David E. Park III Brian D. Patterson USA-International Foundation The Bill and Stephanie Sick Fund Steven L. Smith & Diana T. Go Stand Together Robin and Sandy Stuart Foundation The Stuart Family Foundation Tammy and Guillermo Treviño Valhalla Charitable Foundation White Star Education Foundation Schmidt Futures William Marsh Rice University  ',\n",
              "  'obin and Sandy Stuart Foundation The Stuart Family Foundation Tammy and Guillermo Treviño Valhalla Charitable Foundation White Star Education Foundation Schmidt Futures William Marsh Rice University    \\n',\n",
              "  'Study where you want, what \\nyou want, when you want. \\nAccess. The future of education.\\nopenstax.org          When you access your book in our web view, you can use our new online\\nhighlighting and note-taking features to create your own study guides.\\nOur books are free and flexible, forever. \\nGet started at openstax.org/details/books/neuroscience\\n',\n",
              "  'Cont ents\\nPrefac e 1\\nCHAP TER 1\\nStructur e and F unction o f the Ner vous Sy stem: Cel ls and\\nAna tomy 9\\nIntroduction 9\\n1.1Building a Ner vous Sy stem 10\\n1.2Organization o f the Ner vous Sy stem 21\\n1.3The Centr al Ner vous Sy stem: CNS 30\\n1.4The Br ain: Structur e and F unction 36\\n1.5The P eripher al Ner vous Sy stem: PNS 46\\nSection Summar y 52\\nKey Terms 52\\nReferences 53\\nMultiple Choic e 55\\nFill in the Blank 58\\nCHAP TER 2\\nNeur ophysiolog y 59\\nIntroduction 59\\n2.1Neur al Communication 60\\n2.2Neur al Cir cuits 69\\n2.3Principles o f Bioelectricity 75\\n2.4Mechanisms o f Neur al Signaling 90\\n2.5Our Deep but Stil l Inc omplet e Unders tanding o f Neur al Signaling 107\\nSection Summar y 116\\nKey Terms 117\\nReferences 117\\nMultiple Choic e 119\\nFill in the Blank 122\\nCHAP TER 3\\nBasic Neur ochemis try 123\\nIntroduction 123\\n3.1Gener al Neur ochemis try Principles 126\\n3.2Neur otransmit ters Made fr om Amino Acids 130\\n3.3Neur otransmit ters Made fr om F ats 148\\nSection Summar y 156\\nKey Terms 156\\nReferences 1',\n",
              "  'troduction 123\\n3.1Gener al Neur ochemis try Principles 126\\n3.2Neur otransmit ters Made fr om Amino Acids 130\\n3.3Neur otransmit ters Made fr om F ats 148\\nSection Summar y 156\\nKey Terms 156\\nReferences 156\\nMultiple Choic e 158\\nFill in the Blank 161'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'chunk_id': '1-0',\n",
              "   'page': 2,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '2-0',\n",
              "   'page': 3,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '2-1',\n",
              "   'page': 3,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '2-2',\n",
              "   'page': 3,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '3-0',\n",
              "   'page': 4,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '3-1',\n",
              "   'page': 4,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '3-2',\n",
              "   'page': 4,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '4-0',\n",
              "   'page': 5,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '5-0',\n",
              "   'page': 6,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913},\n",
              "  {'chunk_id': '5-1',\n",
              "   'page': 6,\n",
              "   'source': 'Introduction_to_Behavioral_Neuroscience-WEB',\n",
              "   'total_pages': 913}],\n",
              " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
              "  <IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSPy Setup for RAG\n",
        "\n",
        "**Approach:** I configured the embedder and retriever in DSPY, using the mistral-embed model, retrieving 5 documents per query.\n",
        "\n",
        "**Problems:**\n",
        "* Claude is pretty out of date with DSPy's current capabilities, so I was not able to rely heavily on generated code for this part.\n",
        "* I had to set a pretty small batch size for the Embedder to accomodate Mistral's token limit, since I didn't want to reduce the chunk size quite yet.\n",
        "\n",
        "**Possible future improvement:** Experiment with different embedding models to optimize for performance, optimize chunk size"
      ],
      "metadata": {
        "id": "dzJGctgKOlqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your RAG application\n",
        "class EducationalQuery(dspy.Signature):\n",
        "    \"\"\"Query an educational assistant about textbook content.\"\"\"\n",
        "    question = dspy.InputField()\n",
        "    context = dspy.InputField(desc=\"Retrieved passages from textbooks\")\n",
        "    answer = dspy.OutputField(desc=\"Comprehensive answer based on the retrieved information\")\n",
        "    sources = dspy.OutputField(desc=\"The sources used to answer the question\")\n",
        "\n",
        "class EducationalAssistant(dspy.Module):\n",
        "    def __init__(self, retriever):\n",
        "        super().__init__()\n",
        "        self.retriever = retriever\n",
        "        self.generate = dspy.ChainOfThought(EducationalQuery)\n",
        "\n",
        "    def forward(self, question):\n",
        "        # Get passages and metadata using a single query\n",
        "        retrieved = self.retriever(question)\n",
        "\n",
        "        # Create context from passages\n",
        "        context = \"\\n\\n\".join(retrieved.passages)\n",
        "\n",
        "        # Extract metadata for citations\n",
        "        citation_strings = []\n",
        "        for metadata in retrieved.metadatas:\n",
        "            if isinstance(metadata, dict) and 'source' in metadata and 'page' in metadata:\n",
        "                source = metadata['source'].replace('_', ' ').replace('-WEB', '')\n",
        "                page = metadata['page']\n",
        "                citation = f\"{source} (Page {page})\"\n",
        "                if citation not in citation_strings:\n",
        "                    citation_strings.append(citation)\n",
        "\n",
        "        # Generate answer\n",
        "        response = self.generate(\n",
        "            question=question,\n",
        "            context=context\n",
        "        )\n",
        "\n",
        "        # Format the citations as a bullet list\n",
        "        citations_formatted = \"\\n\".join([f\"• {citation}\" for citation in citation_strings])\n",
        "\n",
        "        return {\n",
        "            \"answer\": response.answer,\n",
        "            \"citations\": citations_formatted,\n",
        "            \"context\": context\n",
        "        }\n"
      ],
      "metadata": {
        "id": "P-cW5CJbsIwJ"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetadataRetriever:\n",
        "    def __init__(self, collection_name, persist_dir, k=5):\n",
        "        self.collection_name = collection_name\n",
        "        self.persist_dir = persist_dir\n",
        "        self.k = k\n",
        "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
        "        self.collection = self.client.get_collection(collection_name)\n",
        "\n",
        "    def __call__(self, query):\n",
        "        # Query ChromaDB once to get both passages and metadata\n",
        "        results = self.collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=self.k\n",
        "        )\n",
        "\n",
        "        # Create dspy-compatible result object\n",
        "        retrieved_results = type('RetrievedPassages', (), {})()\n",
        "        retrieved_results.passages = results['documents'][0]\n",
        "        retrieved_results.metadatas = results['metadatas'][0]\n",
        "\n",
        "        return retrieved_results"
      ],
      "metadata": {
        "id": "qp9YzMy1s7qv"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the retriever\n",
        "retriever = MetadataRetriever(\n",
        "    collection_name=collection_name,\n",
        "    persist_dir=persist_dir,\n",
        "    k=3\n",
        ")\n",
        "\n",
        "# Initialize the assistant with the retriever\n",
        "assistant = EducationalAssistant(retriever)\n",
        "\n",
        "# Configure DSPy to use the LLM (but not the retriever)\n",
        "dspy.settings.configure(lm=lm)\n",
        "\n",
        "# Test the assistant\n",
        "response = assistant(\"What is cellular respiration?\")\n",
        "print(response[\"answer\"])\n",
        "print(\"\\nCitations:\")\n",
        "print(response[\"citations\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhCoz3_7Mbsh",
        "outputId": "c172312d-4440-4b97-e69b-bff08120c09b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cellular respiration is the process by which cells generate energy from the reaction of oxygen with molecules derived from food. This process is essential for all living organisms as it provides the energy needed for various cellular activities. During cellular respiration, cells consume oxygen and produce carbon dioxide as a waste product. The energy generated is stored in the form of adenosine triphosphate (ATP), which is the primary energy currency of all cells. This energy is then used to power various cellular processes, including muscle contraction and other metabolic activities. The overall reaction of cellular respiration can be summarized as the reverse of photosynthesis, where oxygen is consumed, and carbon dioxide is released.\n",
            "\n",
            "Citations:\n",
            "• Introduction to Behavioral Neuroscience (Page 727)\n",
            "• ConceptsofBiology (Page 125)\n",
            "• ConceptsofBiology (Page 105)\n",
            "• ConceptsofBiology (Page 114)\n",
            "• Introduction to Behavioral Neuroscience (Page 729)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Educational Domain Knowledge"
      ],
      "metadata": {
        "id": "xRM11j4wOwwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "URVVfXD2O7EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Usage Disclosure\n",
        "\n",
        "The following LLM-based assistants were used in the development of this notebook:\n",
        "\n",
        "Claude 3.7 Sonnet for:\n",
        "* Use case brainstorming and dataset selection\n",
        "\n",
        "\n",
        "\n",
        "## Authorship\n",
        "All core components, concepts, and technical implementation of this notebook were authored by Sarah Ostermeier. LLM assistance was limited to the specific tasks listed above.\n"
      ],
      "metadata": {
        "id": "KdZzVVlDL0m4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFKdBWE62KB7"
      },
      "outputs": [],
      "source": []
    }
  ]
}