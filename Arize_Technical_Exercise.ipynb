{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Py9nmahA6_-PdO1RCSoD798ZSic4tz6P",
      "authorship_tag": "ABX9TyMjtZ5QvI0Dvv33hNabayVl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarahOstermeier/TechnicalExercises/blob/main/Arize_Technical_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planning\n",
        "\n",
        "**Objective:**  Build a RAG application\n",
        "\n",
        "## Approach\n",
        "\n",
        "**Format:** Jupyter Notebook (Google Colab)  \n",
        "**Stretch Goal:** Optimize performance (primary), Build UX (secondary)  \n",
        "**Framework:** Langchain or DSPy  \n",
        "**LLM Provider:** Huggingface or Mistral  \n",
        "**Dataset:** [OpenStax](https://openstax.org/subjects)\n",
        "\n",
        "****\n",
        "\n",
        "## Requirements\n",
        "\n",
        "**A working RAG app with some interface for Q&A**  \n",
        "* ~75-80% of the time, 2-3 hours <br>\n",
        "\n",
        "\n",
        "**Thorough documentation**  \n",
        "\n",
        "* Clear setup instructions - make it so anyone can follow in your footsteps\n",
        "* Tell us why you picked your tools\n",
        "* Share what worked, what didn't, and how you dealt with it\n",
        "* What would you do next if you had more time?\n",
        "* ~20-25% of your time, 1 hour\n",
        "\n",
        "****\n",
        "\n",
        "## Tips\n",
        "\n",
        "* Use those quickstart tools - no need to reinvent the wheel\n",
        "* Document as you go - future you will thank you\n",
        "* LLMs are your friend here, donâ€™t be afraid to use them to help, just be sure you take the time to really understand what they tell you.\n",
        "* Hit a wall? Don't spin your wheels - reach out!\n",
        "* Keep it focused - better to nail the basics than half-finish three extra features"
      ],
      "metadata": {
        "id": "rnvVjlbV2yqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to run this notebook"
      ],
      "metadata": {
        "id": "i8JPcqF0NNeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My process\n",
        "\n",
        "## Planning\n",
        "\n",
        "### Appraoch and Tools\n",
        "* I decided to work in Google Colab since it is a tool I am familiar with and will allow me to get started quickly without much setup.\n",
        "* As my RAG framework I chose DSPy, as I'm interested interested in trying out DSPy Optimizers and thought this would be a good opportunity to do so.  \n",
        "* Related to the above, my stretch goal is to optimize performance.\n",
        "* I'll be using HuggingFace or Mistral as my LLM provider, as I already have accounts for both and can access easily.\n",
        "\n",
        "### Use Case and Dataset Selection\n",
        "I started a project on Claude and provided the exercise instructions and the Jupyter Notebook I started as project content. I used Claude to brainstorm project ideas and related datasets and eventually decided to build a RAG tool to query textbooks, using documents from [OpenStax](https://openstax.org/subjects) as my dataset.\n",
        "\n",
        "## Implementation\n",
        "*italicized text*"
      ],
      "metadata": {
        "id": "6y57z-iJJnGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment set up"
      ],
      "metadata": {
        "id": "C3Xx0TJmNVba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import relevant libraries\n",
        "\n",
        "[**DSPy:**](https://dspy.ai/) Framework for RAG application. DSPy provides a \"prompts as code\" library, enabling AI developers to standardize, modularize, and optimize their AI applicatins.\n",
        "\n",
        "**PyDF2:** To extract text from PDFs\n",
        "\n"
      ],
      "metadata": {
        "id": "6ctR71kIIQ-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install dspy\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCeCZQaDUF57",
        "outputId": "10294b8a-1180-401a-8adc-d54dd0167645"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: dspy in /usr/local/lib/python3.11/dist-packages (2.6.10)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.4.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from dspy) (1.61.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dspy) (2.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from dspy) (2024.11.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.11/dist-packages (from dspy) (5.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from dspy) (4.67.1)\n",
            "Requirement already satisfied: datasets<3.0.0,>=2.14.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dspy) (2.32.3)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from dspy) (4.2.1)\n",
            "Requirement already satisfied: pydantic~=2.0 in /usr/local/lib/python3.11/dist-packages (from dspy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.5)\n",
            "Requirement already satisfied: magicattr~=0.1.6 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.1.6)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.59.8 in /usr/local/lib/python3.11/dist-packages (from dspy) (1.62.1)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from dspy) (5.6.3)\n",
            "Requirement already satisfied: json-repair in /usr/local/lib/python3.11/dist-packages (from dspy) (0.39.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from dspy) (9.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from dspy) (3.7.1)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from dspy) (0.0.8)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from dspy) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from dspy) (3.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->dspy) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.14.6->dspy) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.14.6->dspy) (6.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (8.1.8)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (8.6.1)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (4.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (1.0.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm<2.0.0,>=1.59.8->dspy) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->dspy) (3.0.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (0.8.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->dspy) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.0->dspy) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dspy) (2025.1.31)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->dspy) (2.0.38)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dspy) (2025.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->dspy) (1.3.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.14.6->dspy) (1.18.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm<2.0.0,>=1.59.8->dspy) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm<2.0.0,>=1.59.8->dspy) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.59.8->dspy) (3.21.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.59.8->dspy) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->dspy) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->dspy) (3.1.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "\n",
        "import langchain\n",
        "import dspy"
      ],
      "metadata": {
        "id": "lTJAiGhMJlu_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model set up\n",
        "In this tutorial I'll be accessing models thorugh [Mistral](https://mistral.ai/) and through Huggingface's ([ Serverless inference API](https://huggingface.co/docs/api-inference/index), both of which can be used for free, with some limitiations. The API calls will be made through **DSPy**, which [integrates with a wide range of model providers](https://dspy.ai/)"
      ],
      "metadata": {
        "id": "oKbOyMZv6xRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Model provider API keys as environment variables."
      ],
      "metadata": {
        "id": "u7DV1JoRyFOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comment out if API keys are not saved in your google colab userdata\n",
        "from google.colab import userdata\n",
        "os.environ[\"MISTRAL_API_KEY\"] = userdata.get('MISTRAL_API_KEY')\n",
        "# os.environ[\"HUGGINGFACE_API_KEY\"] = userdata.get('HUGGINGFACE_API_KEY')\n",
        "\n",
        "## Uncomment and add API keys here if they are not saved in your google colab userdata\n",
        "# os.environ[\"MISTRAL_API_KEY\"] = 'YOUR_MISTRAL_API_KEY'\n",
        "# os.environ[\"HUGGINGFACE_API_KEY\"] = 'YOUR_HUGGINGFACE_API_KEY"
      ],
      "metadata": {
        "id": "4kcTkbYOyCR9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the LLM endpoint with with DSPy."
      ],
      "metadata": {
        "id": "Vx78m6CH5W1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = dspy.LM('mistral/mistral-small-latest', api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
        "dspy.configure(lm=lm)"
      ],
      "metadata": {
        "id": "s4zfgXMJ29W7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the endpoint.\n"
      ],
      "metadata": {
        "id": "BNvZE-3J3dvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lm(messages=[{\"role\": \"user\", \"content\": \"Say this is a test!\"}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiyaMttr3iqZ",
        "outputId": "26ab4ba8-8b72-4b75-e5f5-c683b2739cc4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"This is a test! How can I assist you further? Let's test something if you'd like. How about I say something and you respond with the first word that comes to your mind? I'll start:\\n\\nCat\\n\\n(What word does that make you think of?)\"]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "oFWlcKaFOCkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection and Processing\n",
        "\n",
        "Started by downloading a couple of textbook and access them directly from google drive. Will add web scraping later if there's time\n"
      ],
      "metadata": {
        "id": "k-NRMdl7OH_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process the PDFs\n",
        "\n",
        "**Approach:** Used Claude to generate the functions to go through each file in the directory, extract text content from each page, and format all of the data into a list of dictionaries containing the content and metadata for each page. Metadata such as source(title of the textbook) and page number will be saved to use for citation later.\n",
        "\n",
        "**Possible issue:** This does not preserve chapter/section structure in the textbook or content such as images and structured tables. In a more sophisticated implemenation it might be worth doing some more structured data splitting and including more details such as chapter and section in the metadata."
      ],
      "metadata": {
        "id": "y5iMLQMebYyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_drive_dir = \"/content/drive/MyDrive/Colab Notebooks/Arize RAG Exercise\"\n",
        "project_data_folder = \"Data\""
      ],
      "metadata": {
        "id": "izgiUAooS3BI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file, along with page numbers.\"\"\"\n",
        "    documents = []\n",
        "\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        # Get basic information\n",
        "        title = os.path.basename(pdf_path).replace('.pdf', '')\n",
        "        total_pages = len(pdf_reader.pages)\n",
        "\n",
        "        # Process each page\n",
        "        for page_num in range(total_pages):\n",
        "            # Extract text from the page\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Skip empty pages\n",
        "            if not text or len(text.strip()) < 50:  # Skip pages with litter or no text\n",
        "                continue\n",
        "\n",
        "            # Create a document with metadata for citation\n",
        "            documents.append({\n",
        "                'content': text,\n",
        "                'metadata': {\n",
        "                    'source': title,\n",
        "                    'page': page_num + 1,\n",
        "                    'total_pages': total_pages\n",
        "                }\n",
        "            })\n",
        "\n",
        "    return documents\n",
        "\n",
        "def process_pdfs_in_directory(directory):\n",
        "    \"\"\"Process all PDFs in a directory.\"\"\"\n",
        "    all_documents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            try:\n",
        "                documents = extract_text_from_pdf(file_path)\n",
        "                all_documents.extend(documents)\n",
        "                print(f\"Processed {filename}: {len(documents)} pages extracted\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "    return all_documents"
      ],
      "metadata": {
        "id": "MC22tPkdT-bC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load my documents\n",
        "pdf_directory = os.path.join(project_drive_dir, project_data_folder)\n",
        "documents = process_pdfs_in_directory(pdf_directory)\n",
        "\n",
        "# Save to CSV for later use\n",
        "df = pd.DataFrame(documents)\n",
        "textbook_content_path = os.path.join(project_drive_dir, 'textbook_content.csv')\n",
        "df.to_csv(textbook_content_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lar9TxmVltO",
        "outputId": "3dc8b51f-f33e-4ab1-f31a-b069acc24176"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed ConceptsofBiology-WEB.pdf: 612 pages extracted\n",
            "Processed Introduction_to_Behavioral_Neuroscience-WEB.pdf: 912 pages extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into Chunks for Better Retrieval\n",
        "\n",
        "This function was also generated by Claude. I used langchain here since it has a built-in text splitter and started with the following parameters:\n",
        "* chunk_size: 1000\n",
        "* chunk_overlap: 200"
      ],
      "metadata": {
        "id": "YpdI8C1QbO3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
        "#     \"\"\"Split documents into smaller chunks for better retrieval.\"\"\"\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(\n",
        "#         chunk_size=chunk_size,\n",
        "#         chunk_overlap=chunk_overlap\n",
        "#     )\n",
        "\n",
        "#     chunked_documents = []\n",
        "#     for doc in documents:\n",
        "#         chunks = text_splitter.split_text(doc['content'])\n",
        "#         for i, chunk in enumerate(chunks):\n",
        "#             chunked_documents.append({\n",
        "#                 'content': chunk,\n",
        "#                 'metadata': {\n",
        "#                     **doc['metadata'],\n",
        "#                     'chunk_id': i\n",
        "#                 }\n",
        "#             })\n",
        "\n",
        "#     return chunked_documents\n",
        "\n",
        "# # Split the documents\n",
        "# chunked_docs = split_documents(documents)\n",
        "# chunked_df = pd.DataFrame(chunked_docs)\n",
        "# textbook_chunks_path = os.path.join(project_drive_dir, 'textbook_chunks.csv')\n",
        "# chunked_df.to_csv(textbook_chunks_path , index=False)"
      ],
      "metadata": {
        "id": "uuvRQXPibeMN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DSPy Setup for RAG"
      ],
      "metadata": {
        "id": "dzJGctgKOlqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from dspy.retrievers import Embeddings\n",
        "\n",
        "# Use LangChain's text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "chunks = text_splitter.split_text(\"\\n\\n\".join(documents))\n",
        "\n",
        "# # Initialize DSPy embedder and retriever\n",
        "# embedder = dspy.Embedder('openai/text-embedding-3-small')\n",
        "# retriever = Embeddings(corpus=chunks, embedder=embedder, k=5)\n",
        "\n",
        "# # Configure DSPy\n",
        "# dspy.settings.configure(rm=retriever)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "_EXuGRX8ZxMG",
        "outputId": "ff4144d0-9d0a-4814-8a35-a71ab41aac8a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "sequence item 0: expected str instance, dict found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0a19578a6bdd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# # Initialize DSPy embedder and retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, dict found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Educational Domain Knowledge"
      ],
      "metadata": {
        "id": "xRM11j4wOwwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "URVVfXD2O7EW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Usage Disclosure\n",
        "\n",
        "The following LLM-based assistants were used in the development of this notebook:\n",
        "\n",
        "Claude 3.7 Sonnet for:\n",
        "* Use case brainstorming and dataset selection\n",
        "\n",
        "\n",
        "\n",
        "## Authorship\n",
        "All core components, concepts, and technical implementation of this notebook were authored by Sarah Ostermeier. LLM assistance was limited to the specific tasks listed above.\n"
      ],
      "metadata": {
        "id": "KdZzVVlDL0m4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFKdBWE62KB7"
      },
      "outputs": [],
      "source": []
    }
  ]
}